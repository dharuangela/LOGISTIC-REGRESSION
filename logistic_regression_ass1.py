# -*- coding: utf-8 -*-
"""LOGISTIC REGRESSION Ass1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_xy98ogaOXFN22nsG2-i5xhAp5wcbCA
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('car.csv')

# Display basic information about the dataset
print("Dataset Shape:", df.shape)
print("\nFirst few rows of the dataset:")
print(df.head())

# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Check the data types and summary statistics
print("\nData types of each column:")
print(df.dtypes)

print("\nSummary statistics:")
print(df.describe(include='all'))

# Check class distribution
print("\nClass distribution:")
print(df['Car_Acceptability'].value_counts())
print(df['Car_Acceptability'].value_counts(normalize=True) * 100)

##Data Exploration--------------------------------
# Visualize the distribution of the target variable
plt.figure(figsize=(10, 6))
sns.countplot(x='Car_Acceptability', data=df)
plt.title('Distribution of Car Acceptability')
plt.ylabel('Count')
plt.show()

# Relationship between features and the target
plt.figure(figsize=(15, 10))
for i, feature in enumerate(['Buying_Price', 'Maintenance_Price', 'No_of_Doors', 'Person_Capacity', 'Size_of_Luggage', 'Safety']):
    plt.subplot(2, 3, i+1)
    sns.countplot(x=feature, hue='Car_Acceptability', data=df)
    plt.title(f'{feature} vs Car Acceptability')
    plt.xticks(rotation=45)
    plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

## 3. Preprocessing and Feature Engineering ------------------------------
# Convert categorical variables to numerical using Label Encoding for the target and One-Hot Encoding for features
# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
categorical_features = [col for col in categorical_cols if col != 'Car_Acceptability']

# Create transformers for the pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ],
    remainder='passthrough'
)
# Convert target to numerical using Label Encoder
le = LabelEncoder()
y = le.fit_transform(df['Car_Acceptability'])
print("\nEncoded target classes:", le.classes_)

# Seperating the data
X = df.drop('Car_Acceptability', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

## 4. Logistic Regression Construction--------------------
# Create a pipeline with preprocessing and logistic regression
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42, max_iter=1000))
])

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)
y_pred_proba = pipeline.predict_proba(X_test)

## 5. Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("\nModel Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Display confusion matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Display classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# ROC (Reveiver Operating Characters) curves for multiclass (one-vs-rest)
plt.figure(figsize=(12, 8))
for i, class_name in enumerate(le.classes_):
    # Create binary labels for the current class (one-vs-rest)
    y_test_binary = (y_test == i).astype(int)
    y_pred_proba_class = y_pred_proba[:, i]

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_proba_class)

    # Calculate AUC
    roc_auc = roc_auc_score(y_test_binary, y_pred_proba_class)

# Plot ROC curve
plt.plot(fpr, tpr, label=f'[class_name] (AUC = [roc_auc:.2f])')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves (One-vs-Rest)')
plt.legend(loc="lower right")
plt.show()

## 6. Hyperparameter Tuning and Model Optimization---------------
# Define hyperparameters for grid search
param_grid = {
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__solver': ['liblinear', 'saga'],
    'classifier__penalty': ['l1', 'l2']
}

# Perform grid search
grid_search = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

# Print the best parameters
print("\nBest Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

# Evaluate the optimized model
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

print("\nOptimized Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("Classification Report:")
print(classification_report(y_test, y_pred_best, target_names=le.classes_))

# Cross-validation of the final model
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')
print("\nCross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())
print("Standard Deviation:", cv_scores.std())

# Extract feature names from the preprocessor
one_hot_encoder = pipeline.named_steps['preprocessor'].transformers_[0][1]
feature_names = one_hot_encoder.get_feature_names_out(categorical_features)

# Get coefficients from the logistic regression model
coefficients = best_model.named_steps['classifier'].coef_

# Plot feature importance for each class
plt.figure(figsize=(15, 10))
for i, class_name in enumerate(le.classes_):
    plt.subplot(2, 2, i+1)
    coef_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefficients[i]
    })
    coef_df = coef_df.sort_values('Coefficient', ascending=False)

    sns.barplot(x='Coefficient', y='Feature', data=coef_df)
    plt.title(f'Feature Importance for {class_name}')
    plt.tight_layout()

plt.show()

...
The logistic regression model achieved an accuracy of 91.04% after optimization.
The model performs best for predicting the following classes:
   - unacc (F1-score: 0.96)
   - vgood (F1-score: 0.89)
   - acc (F1-score: 0.79)
   - good (F1-score: 0.69)
...